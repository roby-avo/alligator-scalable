{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bb1a083-8502-41d7-90dc-cc1682ede4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start classifier\n",
      "End classifier\n",
      "End writing\n",
      "56.789581298828125\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import orjson\n",
    "import requests\n",
    "import urllib3\n",
    "import os\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, endpoint, data, column_to_classify):\n",
    "        self._endpoint = endpoint\n",
    "        self._data = data\n",
    "        self._columns_to_classify = column_to_classify\n",
    "\n",
    "    def classify_description(self):\n",
    "        for id_row, row in enumerate(self._data[\"candidates\"]):\n",
    "            for columnn_to_classify in self._columns_to_classify:\n",
    "                target = columnn_to_classify[\"target\"]\n",
    "                columns = columnn_to_classify[\"columns\"]\n",
    "                id_col = self._data[\"header\"].index(target)\n",
    "                candidates = row[id_col]\n",
    "                if len(candidates) == 0:\n",
    "                    continue\n",
    "                candidate = candidates[0]        \n",
    "                key = \" \".join([candidate[\"id\"]]+columns)\n",
    "                categories = cache.get(key, None)\n",
    "                     \n",
    "                if categories is None:\n",
    "                    if columns == [\"name\", \"description\"]:  \n",
    "                        text = candidate[\"name\"] + \" \" + candidate[\"description\"]\n",
    "                    else:\n",
    "                        indexes = [self._data[\"header\"].index(col) for col in columns]\n",
    "                        text = \" \".join([self._data[\"rows\"][id_row][\"data\"][index] for index in indexes])\n",
    "                    text = text.encode('utf-8')\n",
    "                    categories = self._get_categories(text)[\"iptc_categories\"]\n",
    "                    #cache[key] = categories     \n",
    "                candidate[\" \".join(columns)] = categories\n",
    "\n",
    "    def _get_categories(self, data):\n",
    "        response = requests.post(self._endpoint, headers=headers, data=data, verify=False)\n",
    "        result = {\"iptc_categories\": [], \"geo_categories\": []}\n",
    "        if response.status_code == 200:\n",
    "            #print(\"Request was successful\")\n",
    "            #print(\"Response JSON:\")\n",
    "            #print(response.json())\n",
    "            result = response.json()\n",
    "            result = {\"iptc_categories\":result[\"iptc_categories\"], \"geo_categories\":result[\"geo_categories\"]}\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "\n",
    "start = time.time()   \n",
    "print(\"Start classifier\")\n",
    "\n",
    "filename_path = sys.argv[1]\n",
    "# Reading\n",
    "with open(\"data/output5.json\", \"rb\") as f:\n",
    "    input_data = orjson.loads(f.read())\n",
    "\n",
    "with(open(\"s7_expert_ai_api/cache.json\", \"rb\")) as f:\n",
    "    cache = orjson.loads(f.read())\n",
    "cache = {}\n",
    "CLASSIFIER_ENDPOINT = os.environ[\"CLASSIFIER_ENDPOINT\"]\n",
    "\n",
    "try:\n",
    "    classifier = Classifier(CLASSIFIER_ENDPOINT, input_data, input_data[\"services\"][\"classifier\"])\n",
    "    classifier.classify_description()\n",
    "except Exception as e:\n",
    "    print(\"Error with classifier, details:\", str(e))\n",
    "\n",
    "print(\"End classifier\")\n",
    "\n",
    "# Writing\n",
    "with open(\"/tmp/output.json\", \"wb\") as f:\n",
    "    f.write(orjson.dumps(input_data, option=orjson.OPT_INDENT_2))\n",
    "\n",
    "print(\"End writing\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c825e10f-ff3f-4a0b-b083-e0560ab9a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start classifier\n",
      "312\n",
      "End classifier\n",
      "End writing\n",
      "11.277249336242676\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import orjson\n",
    "import requests\n",
    "import urllib3\n",
    "import os\n",
    "import aiohttp\n",
    "import ssl\n",
    "import asyncio\n",
    "import time\n",
    "import httpx\n",
    "import traceback \n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, endpoint, data, columns_to_classify):\n",
    "        self._endpoint = endpoint\n",
    "        self._data = data\n",
    "        self._columns_to_classify = columns_to_classify\n",
    "        self._lock = asyncio.Lock()  # Initialize a new lock\n",
    "\n",
    "    async def classify_description(self):\n",
    "        tasks = []\n",
    "        for id_row, row in enumerate(self._data[\"candidates\"]):\n",
    "            for columnn_to_classify in self._columns_to_classify:\n",
    "                target = columnn_to_classify[\"target\"]\n",
    "                columns = columnn_to_classify[\"columns\"]\n",
    "                id_col = self._data[\"header\"].index(target)\n",
    "                candidates = row[id_col]\n",
    "                if len(candidates) == 0:\n",
    "                    continue\n",
    "                candidate = candidates[0]        \n",
    "                key = \" \".join([candidate[\"id\"]]+columns)\n",
    "               \n",
    "                if columns == [\"name\", \"description\"]:  \n",
    "                    text = candidate[\"name\"] + \" \" + candidate[\"description\"]\n",
    "                else:\n",
    "                    indexes = [self._data[\"header\"].index(col) for col in columns]\n",
    "                    text = \" \".join([self._data[\"rows\"][id_row][\"data\"][index] for index in indexes])\n",
    "                text = text.encode('utf-8')\n",
    "                tasks.append((text, candidate, columns))\n",
    "                \n",
    "        # Check if tasks list is empty, and skip running requests if it is\n",
    "        if not tasks:\n",
    "            return []\n",
    "        print(len(tasks))\n",
    "        responses = await self.run_all_requests(tasks)\n",
    "        return responses\n",
    "\n",
    "    async def send_request(self, session, candidate, columns, data):\n",
    "        key = \" \".join([candidate[\"id\"]] + columns + [self._generate_short_fingerprint(str(data))])\n",
    "        categories = cache.get(key)\n",
    "        if categories is None:\n",
    "            async with session.post(self._endpoint, headers=headers, data=data, ssl=False) as response:\n",
    "                response_json = await response.json()\n",
    "                # Update the cache while the lock is held\n",
    "                async with self._lock:  # Acquire the lock before accessing the cache\n",
    "                    cache[key] = response_json[\"iptc_categories\"]\n",
    "            candidate[\" \".join(columns)] = self._cache[key]\n",
    "        else:\n",
    "            candidate[\" \".join(columns)] = categories\n",
    "\n",
    "    async def run_all_requests(self, tasks_data):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = [self.send_request(session, candidate, columns, text) for text, candidate, columns in tasks_data]\n",
    "            responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            return responses\n",
    "\n",
    "\n",
    "    def _generate_short_fingerprint(self, text, length=8):\n",
    "        # We're using MD5 here for simplicity. For more collision resistance you can use SHA256\n",
    "        hash_object = hashlib.md5(text.encode('utf-8'))\n",
    "        # Truncate the hash to the desired length\n",
    "        return hash_object.hexdigest()[:length]\n",
    "    \n",
    "start = time.time() \n",
    "print(\"Start classifier\")\n",
    "\n",
    "filename_path = sys.argv[1]\n",
    "# Reading\n",
    "with open(\"data/output5.json\", \"rb\") as f:\n",
    "    input_data = orjson.loads(f.read())\n",
    "\n",
    "with(open(\"./s7_expert_ai_api/cache.json\", \"rb\")) as f:\n",
    "    cache = orjson.loads(f.read())\n",
    "cache = {}\n",
    "CLASSIFIER_ENDPOINT = os.environ[\"CLASSIFIER_ENDPOINT\"]\n",
    "\n",
    "try:\n",
    "    classifier = Classifier(CLASSIFIER_ENDPOINT, input_data, input_data[\"services\"][\"classifier\"])\n",
    "    responses = await classifier.classify_description()\n",
    "except Exception as e:\n",
    "    print(\"Error with classifier, details:\", str(e), traceback.print_exc())\n",
    "\n",
    "print(\"End classifier\")\n",
    "\n",
    "# Writing\n",
    "with open(\"/tmp/output.json\", \"wb\") as f:\n",
    "    f.write(orjson.dumps(input_data, option=orjson.OPT_INDENT_2))\n",
    "\n",
    "print(\"End writing\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "947c82fc-e26a-4f3d-9823-878fe62c1a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start features extraction\n",
      "Finish features extraction\n",
      "Finish writing\n",
      "6.943553447723389\n"
     ]
    }
   ],
   "source": [
    "import metrics as metrics\n",
    "import os\n",
    "from lamAPI import LamAPI\n",
    "import sys\n",
    "import orjson\n",
    "import time\n",
    "\n",
    "cache_obj = {}\n",
    "cache_lit = {}\n",
    "\n",
    "class FeaturesExtraction:\n",
    "    def __init__(self, data, lamAPI):\n",
    "        self._data = data\n",
    "        self._lamAPI = lamAPI\n",
    "    \n",
    "    \n",
    "    def compute_features(self):\n",
    "        rows = self._data[\"rows\"]\n",
    "        target = self._data[\"target\"]\n",
    "        for index, row in enumerate(rows):\n",
    "            cells = row[\"data\"]\n",
    "            for id_col_ne_subj in target[\"NE\"]:\n",
    "                for id_col_ne_obj in target[\"NE\"]:\n",
    "                    if id_col_ne_subj == id_col_ne_obj:\n",
    "                        continue\n",
    "                    self._compute_similarity_between_ne_cells(index, id_col_ne_subj, id_col_ne_obj)\n",
    "                for id_col_lit_obj in target[\"LIT\"]:\n",
    "                    lit_cell_obj = cells[id_col_lit_obj]\n",
    "                    self._match_lit_cells(index, id_col_ne_subj, id_col_ne_obj, lit_cell_obj, target[\"LIT_DATATYPE\"][str(id_col_lit_obj)])\n",
    "        self._extract_features()\n",
    "        \n",
    "        \n",
    "    def _extract_features(self):\n",
    "        features = [[] for id_col in range(len(self._data[\"metadata\"][\"column\"]))]\n",
    "        for row in self._data[\"candidates\"]:\n",
    "            for id_col, candidates in enumerate(row):\n",
    "                for candidate in candidates:\n",
    "                    features[id_col].append(list(candidate[\"features\"].values()))\n",
    "        self._data[\"features\"] = features\n",
    "    \n",
    "    def _compute_similarity_between_ne_cells(self, id_row, id_col_subj_cell, id_col_obj_cell):\n",
    "        subj_candidates = self._data[\"candidates\"][id_row][id_col_subj_cell]\n",
    "        obj_candidates = self._data[\"candidates\"][id_row][id_col_obj_cell]\n",
    "        subj_id_candidates = [candidate[\"id\"] for candidate in subj_candidates if candidate[\"id\"] not in cache_obj]\n",
    "        obj_id_candidates = [candidate[\"id\"] for candidate in obj_candidates]\n",
    "        \n",
    "        if len(subj_id_candidates) > 0:\n",
    "            prin(len(subj_id_candidates))\n",
    "            subjects_objects = self._lamAPI.objects(subj_id_candidates)\n",
    "\n",
    "        object_rel_score_buffer = {}\n",
    "\n",
    "        for subj_candidate in subj_candidates:\n",
    "            id_subject = subj_candidate[\"id\"]\n",
    "            #subj_candidate_objects = subjects_objects.get(id_subject, {}).get(\"objects\", {})\n",
    "            #cache_obj[id_subject] = subj_candidate_objects\n",
    "            if id_subject not in cache_obj:\n",
    "                subj_candidate_objects = subjects_objects.get(id_subject, {}).get(\"objects\", {})\n",
    "            else:    \n",
    "                subj_candidate_objects = cache_obj.get(id_subject, {})\n",
    "            objects_set = set(subj_candidate_objects.keys())\n",
    "            #subj_candidate[\"matches\"][str(id_col_obj_cell)] = []\n",
    "            #subj_candidate[\"pred\"][str(id_col_obj_cell)] = {}\n",
    "              \n",
    "            objects_itersection = objects_set.intersection(set(obj_id_candidates))\n",
    "            #print(objects_itersection)\n",
    "            obj_score_max = 0\n",
    "            for obj_candidate in obj_candidates:\n",
    "                id_object = obj_candidate[\"id\"]  \n",
    "                if id_object not in objects_itersection:\n",
    "                    continue\n",
    "                              \n",
    "                score = obj_candidate[\"features\"][\"ed_score\"]\n",
    "                if score > obj_score_max:\n",
    "                    obj_score_max = score\n",
    "                   \n",
    "                if id_object not in object_rel_score_buffer:\n",
    "                    object_rel_score_buffer[id_object] = 0\n",
    "                score_rel = subj_candidate[\"features\"][\"ed_score\"]\n",
    "                if score_rel > object_rel_score_buffer[id_object]:\n",
    "                    object_rel_score_buffer[id_object] = score_rel\n",
    "                for predicate in subj_candidate_objects[id_object]:\n",
    "                    subj_candidate[\"matches\"][str(id_col_obj_cell)].append({\n",
    "                        \"p\": predicate,\n",
    "                        \"o\": id_object,\n",
    "                        \"s\": round(score, 3)\n",
    "                    })\n",
    "                    subj_candidate[\"predicates\"][str(id_col_obj_cell)][predicate] = score\n",
    "            subj_candidate[\"features\"][\"p_subj_ne\"] += obj_score_max          \n",
    "        \n",
    "        for obj_candidate in obj_candidates:\n",
    "            id_object = obj_candidate[\"id\"]  \n",
    "            if id_object not in object_rel_score_buffer:\n",
    "                continue\n",
    "            obj_candidate[\"features\"][\"p_obj_ne\"] += object_rel_score_buffer[id_object]    \n",
    "        \n",
    "      \n",
    "    def _match_lit_cells(self, id_row, id_col_subj_cell, id_col_obj_col, obj_cell, obj_cell_datatype):\n",
    "    \n",
    "        def get_score_based_on_datatype(valueInCell, valueFromKG, datatype):\n",
    "            score = 0\n",
    "            if datatype == \"NUMBER\":\n",
    "                score = metrics.compute_similarty_between_numbers(valueInCell, valueFromKG.lower())\n",
    "            elif datatype == \"DATETIME\":\n",
    "                score = metrics.compute_similarity_between_dates(valueInCell, valueFromKG.lower())\n",
    "            elif datatype == \"STRING\":\n",
    "                score = metrics.compute_similarity_between_string(valueInCell, valueFromKg.lower())\n",
    "            return score\n",
    "        \n",
    "        subj_candidates = self._data[\"candidates\"][id_row][id_col_subj_cell]\n",
    "        subj_id_candidates = [candidate[\"id\"] for candidate in subj_candidates if candidate[\"id\"] not in cache_lit]\n",
    "        if len(subj_id_candidates) > 0:\n",
    "            cand_lamapi_literals = self._lamAPI.literals(subj_id_candidates)\n",
    "            if len(cand_lamapi_literals) == 0:\n",
    "                return\n",
    "        \n",
    "        datatype = obj_cell_datatype\n",
    "        \n",
    "        for subj_candidate in subj_candidates:\n",
    "            id_subject = subj_candidate[\"id\"]\n",
    "            #literals = cand_lamapi_literals[id_subject]\n",
    "            if id_subject not in cache_lit:\n",
    "                prin(\"NOT cache!\")\n",
    "                literals = cand_lamapi_literals.get(id_subject, {})\n",
    "            else:   \n",
    "                literals = cache_lit.get(id_subject, {})\n",
    "            if \"literals\" in literals:\n",
    "                literals = literals['literals']    \n",
    "            #cache_lit[id_subject] = literals    \n",
    "            if len(literals[datatype]) == 0:\n",
    "                continue\n",
    "            #subj_candidate[\"matches\"][str(id_col_obj_col)] = []\n",
    "            #subj_candidate[\"pred\"][str(id_col_obj_col)] = {}\n",
    "            #subj_cell.candidates_entities()[subject][\"match_count\"][\"lit\"] += 1\n",
    "            max_score = 0\n",
    "            for predicate in literals[datatype]:\n",
    "                for valueFromKg in literals[datatype][predicate]:\n",
    "                    score = get_score_based_on_datatype(obj_cell, valueFromKg, datatype)\n",
    "                    score = round(score, 3)\n",
    "                    if score > 0:\n",
    "                        subj_candidate[\"matches\"][str(id_col_obj_col)].append({\n",
    "                            \"p\": predicate,\n",
    "                            \"o\": valueFromKg,\n",
    "                            \"s\": round(score, 3)\n",
    "                        })  \n",
    "                        if score > max_score:\n",
    "                            max_score = score\n",
    "                        if predicate not in subj_candidate[\"predicates\"][str(id_col_obj_col)]:\n",
    "                            subj_candidate[\"predicates\"][str(id_col_obj_col)][predicate] = 0\n",
    "                        if score > subj_candidate[\"predicates\"][str(id_col_obj_col)][predicate]:\n",
    "                            subj_candidate[\"predicates\"][str(id_col_obj_col)][predicate] = score    \n",
    "                            \n",
    "            subj_candidate[\"features\"][\"p_subj_lit\"] += max_score\n",
    "            subj_candidate[\"features\"][\"p_subj_lit\"] = round(subj_candidate[\"features\"][\"p_subj_lit\"], 3)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print(\"Start features extraction\")\n",
    "\n",
    "LAMAPI_HOST, LAMAPI_PORT = os.environ[\"LAMAPI_ENDPOINT\"].split(\":\")\n",
    "LAMAPI_TOKEN = os.environ[\"LAMAPI_TOKEN\"]\n",
    "lamAPI = LamAPI(LAMAPI_HOST, LAMAPI_PORT, LAMAPI_TOKEN)\n",
    "filename_path = sys.argv[1]\n",
    "\n",
    "# Reading\n",
    "with open(\"data/output1.json\", \"rb\") as f:\n",
    "    input_data = orjson.loads(f.read())\n",
    "\n",
    "with(open(\"./s3_features_extraction/cache_obj.json\", \"rb\")) as f:\n",
    "    cache_obj = orjson.loads(f.read())\n",
    "\n",
    "with(open(\"./s3_features_extraction/cache_lit.json\", \"rb\")) as f:\n",
    "    cache_lit = orjson.loads(f.read())\n",
    "\n",
    "FeaturesExtraction(input_data, lamAPI).compute_features()\n",
    "\n",
    "print(\"Finish features extraction\")\n",
    "\n",
    "# Writing\n",
    "with open(\"/tmp/output.json\", \"wb\") as f:\n",
    "    f.write(orjson.dumps(input_data, option=orjson.OPT_INDENT_2))\n",
    "\n",
    "print(\"Finish writing\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68d05a8d-f029-4afe-ad8c-fba4bbeedaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f3a85527-68a9-4dd9-b30d-88836eda0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cache.json\", \"wb\") as f:\n",
    "    f.write(orjson.dumps(cache, option=orjson.OPT_INDENT_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "838f4f1e-ca94-49d2-9833-01c2d8c5b218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7cd540d1'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_short_fingerprint(text, length=8):\n",
    "    # We're using MD5 here for simplicity. For more collision resistance you can use SHA256\n",
    "    hash_object = hashlib.md5(text.encode('utf-8'))\n",
    "    # Truncate the hash to the desired length\n",
    "    return hash_object.hexdigest()[:length]\n",
    "\n",
    "\" \".join([generate_short_fingerprint(\"batman begins\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4658b5c-6e84-40ca-8485-8997b5ebb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[\"candidates\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4d6ff5d-f1ad-437f-b59c-80cb5fbd15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start classifier\n",
      "End classifier\n",
      "End writing\n",
      "19.25173568725586\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import orjson\n",
    "import requests\n",
    "import urllib3\n",
    "import os\n",
    "import aiohttp\n",
    "import ssl\n",
    "import asyncio\n",
    "import time\n",
    "import httpx\n",
    "import traceback \n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, endpoint, data, columns_to_classify):\n",
    "        self._endpoint = endpoint\n",
    "        self._data = data\n",
    "        self._columns_to_classify = columns_to_classify\n",
    "\n",
    "    async def classify_description(self):\n",
    "        tasks = []\n",
    "        for id_row, row in enumerate(self._data[\"candidates\"]):\n",
    "            for columnn_to_classify in self._columns_to_classify:\n",
    "                target = columnn_to_classify[\"target\"]\n",
    "                columns = columnn_to_classify[\"columns\"]\n",
    "                id_col = self._data[\"header\"].index(target)\n",
    "                candidates = row[id_col]\n",
    "                if len(candidates) == 0:\n",
    "                    continue\n",
    "                candidate = candidates[0]        \n",
    "                key = \" \".join([candidate[\"id\"]]+columns)\n",
    "                categories = cache.get(key, None)\n",
    "                     \n",
    "                if categories is None:\n",
    "                    if columns == [\"name\", \"description\"]:  \n",
    "                        text = candidate[\"name\"] + \" \" + candidate[\"description\"]\n",
    "                    else:\n",
    "                        indexes = [self._data[\"header\"].index(col) for col in columns]\n",
    "                        text = \" \".join([self._data[\"rows\"][id_row][\"data\"][index] for index in indexes])\n",
    "                    text = text.encode('utf-8')\n",
    "                    tasks.append(asyncio.create_task(self._compute_categories(text, candidate, columns)))\n",
    "                else:\n",
    "                    candidate[\" \".join(columns)] = categories\n",
    "                    \n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "                    \n",
    "    async def _compute_categories(self, data, candidate, columns): \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            try:\n",
    "                async with session.post(self._endpoint, headers=headers, data=data, ssl=False) as response:\n",
    "                    key = \" \".join([candidate[\"id\"]]+columns)\n",
    "                    categories = cache.get(key)\n",
    "                    if categories is None:\n",
    "                        response_json = await response.json()\n",
    "                        cache[\" \".join([candidate[\"id\"]]+columns)] = response_json[\"iptc_categories\"]\n",
    "                        candidate[\" \".join(columns)] = response_json[\"iptc_categories\"]\n",
    "                    else:\n",
    "                        candidate[\" \".join(columns)] = categories\n",
    "                    return    \n",
    "            except Exception as e:\n",
    "                print(f\"Request failed: {e}\")\n",
    "                return None\n",
    "    \n",
    "    \n",
    "\n",
    "start = time.time() \n",
    "print(\"Start classifier\")\n",
    "\n",
    "filename_path = sys.argv[1]\n",
    "# Reading\n",
    "with open(\"data/output5.json\", \"rb\") as f:\n",
    "    input_data = orjson.loads(f.read())\n",
    "\n",
    "with(open(\"./s7_expert_ai_api/cache.json\", \"rb\")) as f:\n",
    "    cache = orjson.loads(f.read())\n",
    "cache = {}\n",
    "CLASSIFIER_ENDPOINT = os.environ[\"CLASSIFIER_ENDPOINT\"]\n",
    "\n",
    "try:\n",
    "    classifier = Classifier(CLASSIFIER_ENDPOINT, input_data, input_data[\"services\"][\"classifier\"])\n",
    "    await classifier.classify_description()\n",
    "except Exception as e:\n",
    "    print(\"Error with classifier, details:\", str(e), traceback.print_exc())\n",
    "\n",
    "print(\"End classifier\")\n",
    "\n",
    "# Writing\n",
    "with open(\"/tmp/output.json\", \"wb\") as f:\n",
    "    f.write(orjson.dumps(input_data, option=orjson.OPT_INDENT_2))\n",
    "\n",
    "print(\"End writing\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b8e78f-0911-4a5e-b377-3e2af65ea9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try lookup for cell: allerdale borough council\n",
      "Try lookup for cell: be first (regeneration) limited\n",
      "Try lookup for cell: birmingham city council\n",
      "Try lookup for cell: birmingham city council\n",
      "Try lookup for cell: birmingham city council\n",
      "Try lookup for cell: birmingham city council\n",
      "Try lookup for cell: birmingham city council\n",
      "Try lookup for cell: blackpool council\n",
      "Try lookup for cell: bolsover district council\n",
      "Try lookup for cell: bradford metropolitan district council\n",
      "Try lookup for cell: city of stoke-on-trent\n",
      "Try lookup for cell: city of york council\n",
      "Try lookup for cell: cleeve school & sixth form centre of excellence\n",
      "Try lookup for cell: cpd - construction division\n",
      "Try lookup for cell: cpd - construction division\n",
      "Try lookup for cell: defra network etendering portal\n",
      "Try lookup for cell: derby city council\n",
      "Try lookup for cell: derby city council\n",
      "Try lookup for cell: derby city council\n",
      "Try lookup for cell: derby city council\n",
      "Try lookup for cell: derbyshire county council\n",
      "Try lookup for cell: doncaster mbc\n",
      "Try lookup for cell: driver and vehcile licensing agency\n",
      "Try lookup for cell: dudley metropolitan borough council\n",
      "Try lookup for cell: east kent hospitals university\n",
      "Try lookup for cell: elmbridge borough council\n",
      "Try lookup for cell: falkirk council\n",
      "Try lookup for cell: hampshire county council\n",
      "Try lookup for cell: hastings borough council\n",
      "Try lookup for cell: hereford &amp; worcester fire and rescue service\n",
      "Try lookup for cell: herefordshire council\n",
      "Try lookup for cell: horsham district council\n",
      "Try lookup for cell: isle of wight council\n",
      "Try lookup for cell: kent county council\n",
      "Try lookup for cell: kirklees council\n",
      "Try lookup for cell: kirklees council\n",
      "Try lookup for cell: l&amp;q construction\n",
      "Try lookup for cell: lancashire county council\n",
      "Try lookup for cell: lincolnshire county council\n",
      "Try lookup for cell: london borough of bromley\n",
      "Try lookup for cell: manchester city council\n",
      "Try lookup for cell: manchester city council\n",
      "Try lookup for cell: manchester metropolitan university\n",
      "Try lookup for cell: met office\n",
      "Try lookup for cell: mse group\n",
      "Try lookup for cell: n e l commissioning support unit\n",
      "Try lookup for cell: n e l commissioning support unit\n",
      "Try lookup for cell: nhs wales shared services partnership\n",
      "Try lookup for cell: north east lincolnshire council\n",
      "Try lookup for cell: north east lincolnshire council\n",
      "Try lookup for cell: north east lincolnshire council\n",
      "Try lookup for cell: north east lincolnshire council\n",
      "Try lookup for cell: north of england commissioning support unit\n",
      "Try lookup for cell: nuneaton &amp; bedworth community enterprises limited (nab)\n",
      "Try lookup for cell: peabody trust\n",
      "Try lookup for cell: places for people group limited\n",
      "Try lookup for cell: royal borough of kensington and chelsea\n",
      "Try lookup for cell: sandwell metropolitan borough council\n",
      "Try lookup for cell: solihull mbc (sol)\n",
      "Try lookup for cell: southend-on-sea borough council\n",
      "Try lookup for cell: southend-on-sea borough council\n",
      "Try lookup for cell: southport &amp; ormskirk hospital\n",
      "Try lookup for cell: south tyneside council\n",
      "Try lookup for cell: south yorkshire mayoral combined authority group\n",
      "Try lookup for cell: sport england\n",
      "Try lookup for cell: swindon borough council\n",
      "Try lookup for cell: thanet district council\n",
      "Try lookup for cell: the abbeyfield society\n",
      "Try lookup for cell: the abbeyfield society\n",
      "Try lookup for cell: the city of liverpool college\n",
      "Try lookup for cell: thirteen housing group ltd\n",
      "Try lookup for cell: transport for greater manchester\n",
      "Try lookup for cell: tyne and wear fire and rescue service (twfrs)\n",
      "Try lookup for cell: united kingdom atomic energy authority\n",
      "Try lookup for cell: walsall council\n",
      "Try lookup for cell: walsall council\n",
      "Try lookup for cell: warrington borough council\n",
      "Try lookup for cell: warwickshire county council (wcc)\n",
      "Try lookup for cell: welwyn hatfield borough council\n",
      "Try lookup for cell: west sussex county council\n",
      "Try lookup for cell: wigan council\n",
      "Try lookup for cell: wigan council\n",
      "Try lookup for cell: worcestershire acute hospitals nhs trust\n",
      "Try lookup for cell: worcestershire acute hospitals nhs trust\n",
      "Try lookup for cell: worcestershire county council\n",
      "Try lookup for cell: worcestershire county council\n",
      "Try lookup for cell: nobel house\n",
      "Try lookup for cell: derby\n",
      "Try lookup for cell: birmingham city council.\n",
      "Try lookup for cell: cleeve school and sixth form centre of excellence\n",
      "Try lookup for cell: derby council house\n",
      "Try lookup for cell: chesterfield\n",
      "Try lookup for cell: city of bradford metropolitan district council\n",
      "Try lookup for cell: stoke-on-trent city council\n",
      "Try lookup for cell: birmingham\n",
      "Try lookup for cell: birmingham city council.\n",
      "Try lookup for cell: blackpool borough council\n",
      "Try lookup for cell: derby\n",
      "Try lookup for cell: winchester\n",
      "Try lookup for cell: falkirk\n",
      "Try lookup for cell: esher\n",
      "Try lookup for cell: derby\n",
      "Try lookup for cell: dvla\n",
      "Try lookup for cell: hereford\n",
      "Try lookup for cell: hereford\n",
      "Try lookup for cell: doncaster council\n",
      "Try lookup for cell: hastings\n",
      "Try lookup for cell: matlock\n",
      "Try lookup for cell: kent and canterbury hospital\n",
      "Try lookup for cell: lincoln\n",
      "Try lookup for cell: newport\n",
      "Try lookup for cell: kirklees council huddersfield customer service centre\n",
      "Try lookup for cell: p & l properties\n",
      "Try lookup for cell: preston\n",
      "Try lookup for cell: civic centre\n",
      "Try lookup for cell: bromley civic centre\n",
      "Try lookup for cell: horsham\n",
      "Try lookup for cell: kent county council - maidstone\n",
      "Try lookup for cell: dudley\n",
      "Try lookup for cell: grimsby\n",
      "Try lookup for cell: manchester\n",
      "Try lookup for cell: nhs nel csu\n",
      "Try lookup for cell: exeter\n",
      "Try lookup for cell: manchester\n",
      "Try lookup for cell: engie\n",
      "Try lookup for cell: manchester outer ring road\n",
      "Try lookup for cell: britannia business park\n",
      "Try lookup for cell: kensington town hall\n",
      "Try lookup for cell: sandwell m b c\n",
      "Try lookup for cell: necs, john snow house\n",
      "Try lookup for cell: grimsby\n",
      "Try lookup for cell: southend connexions service\n",
      "Try lookup for cell: south shields\n",
      "Try lookup for cell: chichester\n",
      "Try lookup for cell: thirteen housing\n",
      "Try lookup for cell: the city of liverpool college clarence street\n",
      "Try lookup for cell: wigan\n",
      "Try lookup for cell: st albans\n",
      "Try lookup for cell: uk atomic energy authority\n",
      "Try lookup for cell: welwyn garden city\n",
      "Try lookup for cell: southport\n",
      "Try lookup for cell: southend borough council\n",
      "Try lookup for cell: walsall council school admissions\n",
      "Try lookup for cell: margate\n",
      "Try lookup for cell: development management (planning permission)\n",
      "Try lookup for cell: nuneaton\n",
      "Try lookup for cell: wigan\n",
      "Try lookup for cell: worcester\n",
      "Try lookup for cell: london\n",
      "Try lookup for cell: tyne and wear fire and rescue\n",
      "Try lookup for cell: birmingham\n",
      "Try lookup for cell: derby\n",
      "Try lookup for cell: england\n",
      "Try lookup for cell: bradford\n",
      "Try lookup for cell: west midlands\n",
      "Try lookup for cell: stoke-on-trent\n",
      "Try lookup for cell: west midlands\n",
      "Try lookup for cell: lancashire\n",
      "Try lookup for cell: west midlands\n",
      "Try lookup for cell: herefordshire\n",
      "Try lookup for cell: england\n",
      "Try lookup for cell: cheltenham\n",
      "Try lookup for cell: west sussex\n",
      "Try lookup for cell: warrington borough council building control\n",
      "Try lookup for cell: swindon\n",
      "Try lookup for cell: huddersfield\n",
      "Try lookup for cell: blackpool\n",
      "Try lookup for cell: doncaster\n",
      "Try lookup for cell: lincolnshire\n",
      "Try lookup for cell: derbyshire\n",
      "Try lookup for cell: herefordshire\n",
      "Try lookup for cell: high peak\n",
      "Try lookup for cell: isle of wight\n",
      "Try lookup for cell: hampshire\n",
      "Try lookup for cell: maidstone\n",
      "Try lookup for cell: bromley\n",
      "Try lookup for cell: england\n",
      "Try lookup for cell: west midlands\n",
      "Try lookup for cell: west midlands\n",
      "Try lookup for cell: construction procurement delivery (cpd)\n",
      "Try lookup for cell: york\n",
      "Try lookup for cell: huddersfield\n",
      "Try lookup for cell: workington\n",
      "Try lookup for cell: be first\n",
      "Try lookup for cell: construction procurement delivery (cpd)\n",
      "Try lookup for cell: swansea\n",
      "Try lookup for cell: west bromwich\n",
      "Try lookup for cell: north east lincolnshire\n",
      "Try lookup for cell: greater manchester\n",
      "Try lookup for cell: durham\n",
      "Try lookup for cell: southend-on-sea\n",
      "Try lookup for cell: london\n",
      "Try lookup for cell: greater manchester\n",
      "Try lookup for cell: north east lincolnshire\n",
      "Try lookup for cell: southend-on-sea\n",
      "Try lookup for cell: devon\n",
      "Try lookup for cell: middlesbrough\n",
      "Try lookup for cell: liverpool\n",
      "Try lookup for cell: north east lincolnshire\n",
      "Try lookup for cell: tyne and wear\n",
      "Try lookup for cell: north east lincolnshire\n",
      "Try lookup for cell: nan\n",
      "Try lookup for cell: hertfordshire\n",
      "Try lookup for cell: hertfordshire\n",
      "Try lookup for cell: places for people\n",
      "Try lookup for cell: london\n",
      "Try lookup for cell: cardiff\n",
      "Try lookup for cell: peabody\n",
      "Try lookup for cell: southend-on-sea\n",
      "Try lookup for cell: hertfordshire\n",
      "Try lookup for cell: sheffield city region\n",
      "Try lookup for cell: kent\n",
      "Try lookup for cell: walsall\n",
      "Try lookup for cell: walsall\n",
      "Try lookup for cell: west sussex\n",
      "Try lookup for cell: worcestershire\n",
      "Try lookup for cell: sunderland\n",
      "Try lookup for cell: greater london\n",
      "Try lookup for cell: greater manchester\n",
      "Try lookup for cell: greater manchester\n",
      "Try lookup for cell: alexandra hospital\n",
      "Try lookup for cell: alexandra hospital\n",
      "Try lookup for cell: warwickshire county council\n",
      "Try lookup for cell: worcestershire\n",
      "Try lookup for cell: greater manchester\n",
      "Try lookup for cell: greater london\n",
      "Try lookup for cell: derbyshire\n",
      "Try lookup for cell: surrey\n",
      "Try lookup for cell: scotland\n",
      "Try lookup for cell: england\n",
      "Try lookup for cell: east sussex\n",
      "Try lookup for cell: canterbury\n",
      "Try lookup for cell: england\n",
      "Try lookup for cell: west yorkshire\n",
      "Try lookup for cell: england\n",
      "Try lookup for cell: england\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: west yorkshire\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: west yorkshire\n",
      "Try lookup for cell: nan\n",
      "Try lookup for cell: greater london\n",
      "Try lookup for cell: gloucestershire\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: warrington\n",
      "Try lookup for cell: south yorkshire\n",
      "Try lookup for cell: kent\n",
      "Try lookup for cell: greater london\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: cumbria\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: barking\n",
      "Try lookup for cell: merseyside\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: belfast\n",
      "Try lookup for cell: wales\n",
      "Try lookup for cell: abingdon\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: county durham\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: greater london\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: greater london\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: rhondda cynon taff\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: sheffield\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: warwick\n",
      "Try lookup for cell: redditch\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: united kingdom\n",
      "Try lookup for cell: tyne and wear\n",
      "Try lookup for cell: oxfordshire\n",
      "Try lookup for cell: derbyshire\n",
      "Try lookup for cell: belfast\n",
      "Try lookup for cell: wales\n",
      "Try lookup for cell: solihull metropolitan borough council\n",
      "Try lookup for cell: warwickshire\n",
      "Try lookup for cell: warwickshire\n",
      "Try lookup for cell: northern ireland\n",
      "Try lookup for cell: northern ireland\n",
      "Try lookup for cell: solihull\n",
      "--- 8.507178783416748 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lamAPI3 import LamAPI\n",
    "import sys\n",
    "import json \n",
    "import metrics as metrics\n",
    "import utils as utils\n",
    "import time \n",
    "import asyncio\n",
    "import traceback \n",
    "\n",
    "cache = {}\n",
    "class Lookup:\n",
    "    def __init__(self, data:object, lamAPI):\n",
    "        self._header = data.get(\"header\", [])\n",
    "        self._table_name = data[\"name\"]\n",
    "        self._target = data[\"target\"]\n",
    "        self._kg_ref = data[\"kg_reference\"]\n",
    "        self._limit = data[\"limit\"]\n",
    "        self._lamAPI = lamAPI\n",
    "        self._rows = []\n",
    "        self._cache = {}\n",
    "\n",
    "        \"\"\"\n",
    "        for row in data[\"rows\"]:\n",
    "            row = self._build_row(row[\"data\"])\n",
    "            self._rows.append(row)\n",
    "        \"\"\"\n",
    "        \n",
    "    async def test(self, data):\n",
    "        tasks = []\n",
    "        for row in data[\"rows\"]:\n",
    "            tasks.append(asyncio.create_task(self._build_row(row[\"data\"])))\n",
    "\n",
    "        # Use asyncio.gather() to run _build_row() concurrently\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Collect the results into self._rows\n",
    "        for response in responses:\n",
    "            self._rows.append(response)\n",
    "\n",
    "            \n",
    "    async def _build_row(self, cells):\n",
    "        row_candidates = []\n",
    "        features = [\"ntoken\", \"popularity\", \"pos_score\", \"es_score\", \"es_diff_score\", \n",
    "                    \"ed_score\", \"jaccard_score\", \"jaccardNgram_score\", \"cosine_similarity\",\n",
    "                    \"p_subj_ne\", \"p_subj_lit\", \"p_obj_ne\", \"desc\", \"descNgram\", \n",
    "                    \"cpa\", \"cpaMax\", \"cta\", \"ctaMax\", \"rho\", \"diff\"]\n",
    "        row_content_norm = utils.clean_str(\" \".join(cells))\n",
    "        for i, cell in enumerate(cells):\n",
    "            new_candidites = []\n",
    "            if i in self._target[\"NE\"]:\n",
    "                if cell not in self._cache:\n",
    "                    candidates = await self._get_candidates(cell)\n",
    "                    self._cache[cell] = candidates\n",
    "                else:\n",
    "                    candidates = self._cache[cell]\n",
    "                #print(\"len\", len(candidates))\n",
    "                #candidates = cache.get(cell, [])\n",
    "                #print(cell)\n",
    "                #cache[cell] = candidates\n",
    "                for candidate in candidates:\n",
    "                    item = {\n",
    "                        \"id\": candidate[\"id\"],\n",
    "                        \"name\": candidate[\"name\"],\n",
    "                        \"description\": candidate[\"description\"],\n",
    "                        \"types\": candidate[\"types\"],\n",
    "                        \"features\": {feature:candidate.get(feature, 0) for feature in features},\n",
    "                        \"matches\": {str(id_col):[] for id_col in range(len(cells))},\n",
    "                        \"predicates\": {str(id_col):{} for id_col in range(len(cells))}\n",
    "                    }\n",
    "                    new_candidites.append(item)\n",
    "                    desc_norm = utils.clean_str(candidate[\"description\"])\n",
    "                    desc_score = round(metrics.compute_similarity_between_string(desc_norm, row_content_norm), 3)\n",
    "                    desc_score_ngram = round(metrics.compute_similarity_between_string(desc_norm, row_content_norm, 3), 3)\n",
    "                    item[\"features\"][\"desc\"] = desc_score\n",
    "                    item[\"features\"][\"descNgram\"] = desc_score_ngram\n",
    "            row_candidates.append(new_candidites)\n",
    "        return row_candidates\n",
    "\n",
    "\n",
    "    async def _get_candidates(self, cell):\n",
    "        print(\"Try lookup for cell:\", cell)\n",
    "        candidates = []\n",
    "        types = None\n",
    "        result = None\n",
    "        try:\n",
    "            result = await self._lamAPI.lookup(cell)\n",
    "            #print(len(result))\n",
    "            #print(result)\n",
    "            if cell not in result:\n",
    "                raise Exception(\"Error from lamAPI\")\n",
    "            candidates = result[cell]        \n",
    "        except Exception as e:\n",
    "            print(str(e), traceback.print_exc())\n",
    "            return []\n",
    "            \n",
    "        return candidates\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "SAMPLE_SIZE = 25\n",
    "LAMAPI_HOST, LAMAPI_PORT = os.environ[\"LAMAPI_ENDPOINT\"].split(\":\")\n",
    "LAMAPI_TOKEN = os.environ[\"LAMAPI_TOKEN\"]\n",
    "lamAPI = LamAPI(LAMAPI_HOST, LAMAPI_PORT, LAMAPI_TOKEN)\n",
    "filename_path = sys.argv[1]\n",
    "\n",
    "with open(\"./data/output_l.json\") as f:\n",
    "    input = json.loads(f.read())\n",
    "\n",
    "with(open(\"./cache.json\")) as f:\n",
    "    cache = json.loads(f.read())\n",
    "    \n",
    "p1 = Lookup(input, lamAPI)\n",
    "await p1.test(input)\n",
    "input[\"candidates\"] = p1._rows\n",
    "\n",
    "with open(\"./output_l.json\", \"w\") as f:\n",
    "    f.write(json.dumps(input, indent=4))\n",
    "    \n",
    "#with(open(\"./cache.json\", \"w\")) as f:\n",
    "#    f.write(json.dumps(cache))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effdc868-50d6-4c7e-8d32-8959b1664a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9649a92-8d18-4187-9851-9260d3b1aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lookup\n",
      "End lookup\n",
      "End writing\n",
      "--- 4.374894380569458 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lamAPI import LamAPI\n",
    "import sys\n",
    "import orjson\n",
    "import metrics as metrics\n",
    "import utils as utils\n",
    "import time\n",
    "\n",
    "class Lookup:\n",
    "    def __init__(self, data:object, lamAPI):\n",
    "        self._header = data.get(\"header\", [])\n",
    "        self._table_name = data[\"name\"]\n",
    "        self._target = data[\"target\"]\n",
    "        self._kg_ref = data[\"kg_reference\"]\n",
    "        self._limit = data[\"limit\"]\n",
    "        self._lamAPI = lamAPI\n",
    "        self._setNE = set(self._target[\"NE\"])\n",
    "        self._rows = []\n",
    "        for row in data[\"rows\"]:\n",
    "            row = self._build_row(row[\"data\"])\n",
    "            self._rows.append(row)\n",
    "\n",
    "\n",
    "    def _build_row(self, cells):\n",
    "        row_candidates = []\n",
    "        features = [\"ntoken\", \"popularity\", \"pos_score\", \"es_score\", \"es_diff_score\", \n",
    "                    \"ed_score\", \"jaccard_score\", \"jaccardNgram_score\", \"cosine_similarity\",\n",
    "                    \"p_subj_ne\", \"p_subj_lit\", \"p_obj_ne\", \"desc\", \"descNgram\", \n",
    "                    \"cpa\", \"cpaMax\", \"cta\", \"ctaMax\", \"rho\", \"diff\"]\n",
    "        \n",
    "        #row_content_norm = utils.clean_str(\" \".join(cells))\n",
    "        cells_to_consider = []\n",
    "        for i, cell in enumerate(cells):\n",
    "            if i not in self._target.get(\"NO_ANN\", []):\n",
    "                cells_to_consider.append(cell)\n",
    "        row_content_norm = utils.clean_str(\" \".join(cells_to_consider))        \n",
    "        for i, cell in enumerate(cells):\n",
    "            new_candidites = []\n",
    "            if i in self._setNE:\n",
    "                #candidates = self._get_candidates(cell)\n",
    "                #print(\"Lookup for cell\", cell)\n",
    "                if cell in cache:\n",
    "                    candidates = cache.get(cell, [])\n",
    "                else:\n",
    "                    candidates = self._get_candidates(cell)    \n",
    "\n",
    "                for candidate in candidates:\n",
    "                    item = {\n",
    "                        \"id\": candidate[\"id\"],\n",
    "                        \"name\": candidate[\"name\"],\n",
    "                        \"description\": candidate[\"description\"],\n",
    "                        \"types\": candidate[\"types\"],\n",
    "                        \"features\": {feature:candidate.get(feature, 0) for feature in features},\n",
    "                        \"matches\": {str(id_col):[] for id_col in range(len(cells))},\n",
    "                        \"predicates\": {str(id_col):{} for id_col in range(len(cells))}\n",
    "                    }\n",
    "                    new_candidites.append(item)\n",
    "                    desc_norm = utils.clean_str(candidate[\"description\"])\n",
    "                    desc_score = round(metrics.compute_similarity_between_string(desc_norm, row_content_norm), 3)\n",
    "                    desc_score_ngram = round(metrics.compute_similarity_between_string(desc_norm, row_content_norm, 3), 3)\n",
    "                    item[\"features\"][\"desc\"] = desc_score\n",
    "                    item[\"features\"][\"descNgram\"] = desc_score_ngram\n",
    "            row_candidates.append(new_candidites)\n",
    "        return row_candidates\n",
    "\n",
    "\n",
    "    def _get_candidates(self, cell):\n",
    "        print(\"Try lookup for cell:\", cell)\n",
    "        candidates = []\n",
    "        types = None\n",
    "        result = None\n",
    "        try:\n",
    "            result = self._lamAPI.lookup(cell, fuzzy=False, types=types, kg=self._kg_ref, limit=self._limit)\n",
    "            if cell not in result:\n",
    "                raise Exception(\"Error from lamAPI\")\n",
    "            candidates = result[cell]    \n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return []\n",
    "            \n",
    "        return candidates\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Start lookup\")\n",
    "\n",
    "SAMPLE_SIZE = 25\n",
    "LAMAPI_HOST, LAMAPI_PORT = os.environ[\"LAMAPI_ENDPOINT\"].split(\":\")\n",
    "LAMAPI_TOKEN = os.environ[\"LAMAPI_TOKEN\"]\n",
    "lamAPI = LamAPI(LAMAPI_HOST, LAMAPI_PORT, LAMAPI_TOKEN)\n",
    "filename_path = sys.argv[1]\n",
    "\n",
    "# Reading\n",
    "with open(\"./data/output1.json\", \"rb\") as f:\n",
    "    input_data = orjson.loads(f.read())\n",
    "\n",
    "with(open(\"./s2_lookup/cache.json\", \"rb\")) as f:\n",
    "    cache = orjson.loads(f.read())\n",
    "\n",
    "p1 = Lookup(input_data, lamAPI)\n",
    "input_data[\"candidates\"] = p1._rows\n",
    "\n",
    "print(\"End lookup\")\n",
    "\n",
    "# Writing\n",
    "with open(\"./data/output.json\", \"wb\") as f:\n",
    "    f.write(orjson.dumps(input_data, option=orjson.OPT_INDENT_2))\n",
    "\n",
    "print(\"End writing\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e59f4-f32c-4530-9495-69db974feba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"s1_pre_processing/SN_latest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f7a89-efc0-4120-a1cf-533ab0275a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lamAPI3 import LamAPI\n",
    "SAMPLE_SIZE = 25\n",
    "LAMAPI_HOST, LAMAPI_PORT = os.environ[\"LAMAPI_ENDPOINT\"].split(\":\")\n",
    "LAMAPI_TOKEN = os.environ[\"LAMAPI_TOKEN\"]\n",
    "lamAPI = LamAPI(LAMAPI_HOST, LAMAPI_PORT, LAMAPI_TOKEN)\n",
    "await lamAPI.lookup(\"paris\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64d84096-a572-4bf3-8512-a584ccac4570",
   "metadata": {},
   "source": [
    "--- 1125.024539232254 seconds ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84a428-3eb0-4dc4-87fb-e2a24f4d13aa",
   "metadata": {},
   "source": [
    "--- 1125.024539232254 seconds ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d5a54-8a68-49ea-89c7-91e6097e66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "--- 10.024539232254 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ddd240-c6a1-484a-8058-293c040b52a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 11:34:36.220231: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-20 11:34:36.240973: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-20 11:34:36.408991: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-20 11:34:36.410143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 11:34:37.366507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been read correctly\n",
      "The NN has been read correctly\n",
      "235/235 [==============================] - 0s 959us/step\n",
      "220/220 [==============================] - 0s 930us/step\n",
      "175/175 [==============================] - 0s 996us/step\n",
      "151/151 [==============================] - 0s 919us/step\n",
      "228/228 [==============================] - 0s 1ms/step\n",
      "253/253 [==============================] - 0s 981us/step\n",
      "The NN has been applied correctly\n",
      "The file has been saved correctly\n",
      "--- 5.965457439422607 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class Prediction:\n",
    "    def __init__(self, data, model):\n",
    "        self._data = data\n",
    "        self._model = model\n",
    "        \n",
    "    def compute_prediction(self, feature_name):\n",
    "        prediction = []\n",
    "        indexes = []\n",
    "        for column_features in self._data[\"features\"]:\n",
    "            pred = [] \n",
    "            if len(column_features) > 0:\n",
    "                pred = self._model.predict(column_features)\n",
    "            prediction.append(pred)\n",
    "            indexes.append(0)\n",
    "        \n",
    "        for row in self._data[\"candidates\"]:\n",
    "            for id_col, candidates in enumerate(row):\n",
    "                for candidate in candidates:\n",
    "                    index = indexes[id_col]\n",
    "                    indexes[id_col] += 1\n",
    "                    feature = round(float(prediction[id_col][index][1]), 3)\n",
    "                    if feature_name == \"rho2\": \n",
    "                        candidate[feature_name] = feature\n",
    "                    else:\n",
    "                        candidate[\"features\"][feature_name] = feature    \n",
    "                if feature_name == \"rho2\":        \n",
    "                    candidates.sort(key=lambda x:x[feature_name], reverse=True)       \n",
    "                else:\n",
    "                    candidates.sort(key=lambda x:x[\"features\"][feature_name], reverse=True)    \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "filename_path = sys.argv[1]\n",
    "feature_name = sys.argv[2]\n",
    "\n",
    "with open(\"output_feat_ex.json\") as f:\n",
    "    input = json.loads(f.read())\n",
    "print(\"The file has been read correctly\")\n",
    "\n",
    "model = load_model(\"neural_network.h5\")\n",
    "print(\"The NN has been read correctly\")\n",
    "\n",
    "Prediction(input, model).compute_prediction(feature_name)\n",
    "print(\"The NN has been applied correctly\")\n",
    "\n",
    "with open(\"/tmp/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(input, indent=4))\n",
    "print(\"The file has been saved correctly\")\n",
    "#print(json.dumps(input), flush=True)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95a977c5-e90e-4c65-9e6d-61b10589949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fcc8f30-a0a6-42d3-9b26-e964347ee5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " <function tensorflow.python.framework.config.get_intra_op_parallelism_threads()>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.threading.get_inter_op_parallelism_threads(), tf.config.threading.get_intra_op_parallelism_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e749c8b-d0b7-4b39-9c6d-b1e4ad3770a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import orjson\n",
    "import requests\n",
    "import urllib3\n",
    "import os\n",
    "import time\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, endpoint, data):\n",
    "        self._endpoint = endpoint\n",
    "        self._data = data\n",
    "\n",
    "    def classify_description(self):\n",
    "        for row in self._data[\"candidates\"]:\n",
    "            for candidates in row:\n",
    "                if len(candidates) > 0:\n",
    "                    candidate = candidates[0]\n",
    "                    if candidate[\"id\"] not in cache:\n",
    "                        temp = candidate[\"name\"] + \" \" + candidate[\"description\"]\n",
    "                        temp = temp.encode('utf-8')\n",
    "                        categories = self._get_categories(temp)[\"iptc_categories\"]\n",
    "                        cache[candidate[\"id\"]] = categories\n",
    "                    else:\n",
    "                        categories = cache.get(candidate[\"id\"], [])\n",
    "                    candidate[\"categories\"] = categories\n",
    "                    #candidate[\"iptc_categories\"] = categories[\"iptc_categories\"]\n",
    "                    #candidate[\"geo_categories\"] = categories[\"geo_categories\"]\n",
    "\n",
    "    def _get_categories(self, data):\n",
    "        response = requests.post(self._endpoint, headers=headers, data=data, verify=False)\n",
    "        result = {\"iptc_categories\": [], \"geo_categories\": []}\n",
    "        if response.status_code == 200:\n",
    "            print(\"Request was successful\")\n",
    "            print(\"Response JSON:\")\n",
    "            print(response.json())\n",
    "            result = response.json()\n",
    "            result = {\"iptc_categories\":result[\"iptc_categories\"], \"geo_categories\":result[\"geo_categories\"]}\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n",
    "        return result\n",
    "    \n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start classifier\")\n",
    "\n",
    "filename_path = sys.argv[1]\n",
    "# Reading\n",
    "with open(\"output_pred2.json\", \"rb\") as f:\n",
    "    input_data = orjson.loads(f.read())\n",
    "\n",
    "with(open(\"./s7_expert_ai_api/cache.json\", \"rb\")) as f:\n",
    "    cache = orjson.loads(f.read())\n",
    "\n",
    "CLASSIFIER_ENDPOINT = os.environ[\"CLASSIFIER_ENDPOINT\"]\n",
    "\n",
    "try:\n",
    "    classifier = Classifier(CLASSIFIER_ENDPOINT, input_data)\n",
    "    classifier.classify_description()\n",
    "except Exception as e:\n",
    "    print(\"Error with classifier, details:\", str(e))\n",
    "\n",
    "print(\"End classifier\")\n",
    "\n",
    "# Writing\n",
    "with open(\"/tmp/output.json\", \"wb\") as f:\n",
    "    f.write(orjson.dumps(input_data, option=orjson.OPT_INDENT_2))\n",
    "\n",
    "print(\"End writing\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bffb1d8-1abc-485a-b130-8e173963574b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import orjson\n",
    "with(open(\"./s7_expert_ai_api/cache.json\", \"rb\")) as f:\n",
    "    cache = orjson.loads(f.read())\n",
    "len(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db14127-3d59-4ee4-9f31-8909dfc18a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (3.8.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855145c7-8f3c-4abc-a4bd-250d2c8a43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./s7_expert_ai_api/cache.json\", \"w\") as f:\n",
    "    f.write(json.dumps(cache, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
